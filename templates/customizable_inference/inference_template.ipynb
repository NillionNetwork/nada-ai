{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBKyEZ6JJD12"
      },
      "source": [
        "# Nada AI Inference Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usjn1APJJD13"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NillionNetwork/nada-ai/blob/main/templates/customizable_inference/inference_template.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcWCxWr2JD13"
      },
      "source": [
        "This notebook provides a generic & easily customizable end-to-end template for running AI model inference on the Nillion network.\n",
        "\n",
        "Feel free to customize this template to fit your use case by navigating to the cells annotated by a üìù TODO symbol!\n",
        "\n",
        "We are really excited for developers to build with our SDK, if you have any questions please do reach out to us on:\n",
        "\n",
        "[![Discord](https://img.shields.io/badge/Discord-nillionnetwork-%235865F2?logo=discord)](https://discord.gg/nillionnetwork)\n",
        "[![GitHub Discussions](https://img.shields.io/badge/GitHub_Discussions-NillionNetwork-%23181717?logo=github)](https://github.com/orgs/NillionNetwork/discussions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSDvb7RpJD13"
      },
      "source": [
        "# 1. Set up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26G7oJmJJD14"
      },
      "source": [
        "The boring part!\n",
        "\n",
        "Installs all required dependencies and spins up a local devnet that will run Nada programs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lJGlGcFSJD14"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nada-ai~=0.3.0 seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZhUaKwFIJD14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "import uuid\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import nada_numpy as na\n",
        "from nada_ai.client import TorchClient\n",
        "import pandas as pd\n",
        "import nada_numpy.client as na_client\n",
        "from dotenv import load_dotenv\n",
        "from nillion_python_helpers import create_nillion_client, create_payments_config, get_quote, get_quote_and_pay, pay_with_quote\n",
        "import py_nillion_client as nillion\n",
        "from py_nillion_client import NodeKey, UserKey\n",
        "from cosmpy.aerial.client import LedgerClient\n",
        "from cosmpy.aerial.wallet import LocalWallet\n",
        "from cosmpy.crypto.keypairs import PrivateKey\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "from IPython.core.magic import register_cell_magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PRtnZlvWJD15"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"target/\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GalpJ83JJD15"
      },
      "outputs": [],
      "source": [
        "@register_cell_magic\n",
        "def to_file(line, cell):\n",
        "    \"Writes the content of the cell to a file specified in the line argument.\"\n",
        "    filepath = Path(line.strip())\n",
        "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qNsTlEsEJD15"
      },
      "outputs": [],
      "source": [
        "# Configure telemetry settings\n",
        "enable_telemetry = True  #@param {type:\"boolean\"}\n",
        "my_identifier = \"your-telemetry-identifier\"  #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS5l28BkJD15",
        "outputId": "4f7087b8-758c-46e0-d932-62999d0bfa0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7810  100  7810    0     0  11504      0 --:--:-- --:--:-- --:--:-- 11519\n",
            "\n",
            "nilup has been installed into /root/.nilup/bin and added to your $PATH in /root/.bashrc.\n",
            "\n",
            "Run 'source /root/.bashrc' or start a new terminal session to use nilup.\n",
            "\n",
            "By providing your Ethereum wallet address, you consent to the collection of telemetry data by the Nillion Network.\n",
            "That includes but is not limited to\n",
            "- The version of the SDK you are using\n",
            "- The OS you are using\n",
            "- The Processor Architecture you are using\n",
            "- The SDK binary that you are running and the subcommand\n",
            "- The wallet address you provided\n",
            "- The errors produced by the SDK\n",
            "We collect this data to understand how the software is used, and to better assist you in case of issues.\n",
            "While we will not collect any personal information, we still recommend using a new wallet address that cannot be linked to your identity by any third party.\n",
            "For more information, our privacy policy is available at https://nillion.com/privacy/.\n",
            "Do you consent to the collection of telemetry data? (yes/no)\n",
            "Telemetry data collection enabled\n",
            "Installing SDK bins version latest\n",
            "Downloading latest/nillion-sdk-bins-x86_64-unknown-linux-musl.tar.gz\n",
            "SDK version latest installed\n",
            "SDK version latest set as default\n",
            "Installing SDK bins version 0.4.0\n",
            "Downloading 0.4.0/nillion-sdk-bins-x86_64-unknown-linux-musl.tar.gz\n",
            "SDK version 0.4.0 installed\n",
            "SDK version 0.4.0 set as default\n"
          ]
        }
      ],
      "source": [
        "# Install the nilup tool and then use that to install the Nillion SDK\n",
        "!curl https://nilup.nilogy.xyz/install.sh | bash\n",
        "\n",
        "# Update Path if ran in colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    os.environ[\"PATH\"] += \":/root/.nilup/bin\"\n",
        "    os.environ[\"PATH\"] += \":/root/.nilup/sdks/latest/\"\n",
        "\n",
        "# Set telemetry if opted in\n",
        "if enable_telemetry:\n",
        "    identifier = f\"nada-ai-inference-{str(uuid.uuid4())}-{my_identifier}\"\n",
        "    !echo 'yes' | nilup instrumentation enable --wallet {identifier}\n",
        "\n",
        "# Install the lastest SDK and initialise it\n",
        "!nilup init\n",
        "!nilup install 0.4.0\n",
        "!nilup use 0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPgPBxZCJD15",
        "outputId": "4f80c574-d735-437f-d22d-d6f38731a7eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ],
      "source": [
        "# Spin up local Nillion devnet\n",
        "!nohup nillion-devnet &\n",
        "\n",
        "time.sleep(20)  # Wait for devnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOXX1yzJJD15"
      },
      "source": [
        "# 2. Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF5vxsfRYcHx"
      },
      "source": [
        "We'll start of by loading the dataset we want to train our model on.\n",
        "\n",
        "In this example, we use an AI classic: the Titanic dataset.\n",
        "\n",
        "**TODO: üìù replace this by your own dataset!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "36qV_BdoYcHx"
      },
      "outputs": [],
      "source": [
        "# TODO: üìù read your own dataset!\n",
        "raw_df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    raw_df,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoMElppMYcHx"
      },
      "source": [
        "Next, we will do some pre-processing & general data cleaning.\n",
        "\n",
        "**TODO: üìù (optional) replace this by your own pre-processing!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C9VgMCXWYcHy"
      },
      "outputs": [],
      "source": [
        "# TODO: üìù define your own preprocessing logic here\n",
        "def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    data = data.drop(columns=[\"deck\", \"embark_town\", \"alive\", \"class\", \"who\", \"adult_male\", \"alone\"])\n",
        "    data = data.rename({\"survived\": \"label\"}, axis=1)\n",
        "    data[\"age\"] = data[\"age\"].fillna(data[\"age\"].mean())\n",
        "    data[\"embarked\"] = data[\"embarked\"].fillna(data[\"embarked\"].mode()[0])\n",
        "    data[\"embarked\"] = data[\"embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})\n",
        "    data[\"sex\"] = data[\"sex\"].map({\"male\": 0, \"female\": 1})\n",
        "    scaler = StandardScaler()\n",
        "    X = data.drop(\"label\", axis=1)\n",
        "    y = data[\"label\"]\n",
        "    X = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
        "    data = pd.concat([X, y], axis=1)\n",
        "    return data\n",
        "\n",
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhdqZG0uYcHy"
      },
      "source": [
        "Now, we will transform our dataset into a custom PyTorch Dataset.\n",
        "\n",
        "Here, we assume that:\n",
        "- The dataset consists of only numerical values.\n",
        "- The dataset has a singular column called `label` that is the prediction target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6v859DvQJD15"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        target: str=\"label\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.features = self.data.drop(target, axis=1).values.astype(float)\n",
        "        self.targets = self.data[target].values.astype(float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
        "        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "        return feature, target\n",
        "\n",
        "train_dataset = MyDataset(train_df)\n",
        "test_dataset = MyDataset(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt1p0tPOYcHy"
      },
      "source": [
        "Finally, we wrap our newly created datasets in a PyTorch DataLoader for efficiency reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "au0j0rHuYcHy"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0a3fRuzYcHy"
      },
      "source": [
        "# 3. Create model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot03dwc2YcHy"
      },
      "source": [
        "Now that we have our dataset ready, it's time to define which model we want to use.\n",
        "\n",
        "**Important: we need to ensure that we pick a model architecture that is supported by `nada-ai` and model dimensions that work within the capacity of the network.**\n",
        "You can find more information [here](https://docs.nillion.com/nada-ai-introduction#supported-models)\n",
        "\n",
        "In this example, we will use a small neural net.\n",
        "\n",
        "**TODO: üìù make your own model or load a pre-trained one**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CPlLqmFTJD16"
      },
      "outputs": [],
      "source": [
        "# TODO: üìù make your own model or load a pre-trained one\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.Linear(7, 16)\n",
        "        self.ln2 = nn.Linear(16, 1)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "my_model = MyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Za_qcQEYcHy"
      },
      "source": [
        "# 4. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v1kygWFYcHy"
      },
      "source": [
        "Now that we have both a dataset and a model, the time has come to train our model on the dataset.\n",
        "\n",
        "**Warning**: this example assumes that we are training a model to perform a single-class classification task.\n",
        "If your task differs from this, you will likely need to modify the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFOu6crRJD16",
        "outputId": "faf82ec0-d161-4bce-ab3c-4091523027aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.526559\n",
            "Validation Accuracy: 79.8883%\n",
            "Epoch 10, Loss: 0.395110\n",
            "Validation Accuracy: 79.3296%\n",
            "Epoch 20, Loss: 0.390059\n",
            "Validation Accuracy: 79.8883%\n",
            "Epoch 30, Loss: 0.385221\n",
            "Validation Accuracy: 78.2123%\n",
            "Epoch 40, Loss: 0.378938\n",
            "Validation Accuracy: 78.2123%\n",
            "Epoch 50, Loss: 0.382115\n",
            "Validation Accuracy: 78.7709%\n",
            "Epoch 60, Loss: 0.371655\n",
            "Validation Accuracy: 78.7709%\n",
            "Epoch 70, Loss: 0.368409\n",
            "Validation Accuracy: 78.7709%\n",
            "Epoch 80, Loss: 0.363957\n",
            "Validation Accuracy: 79.8883%\n",
            "Epoch 90, Loss: 0.363746\n",
            "Validation Accuracy: 79.3296%\n"
          ]
        }
      ],
      "source": [
        "training_args = {\n",
        "    \"num_train_epochs\": 100,\n",
        "    \"learning_rate\": 0.01,\n",
        "}\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(\n",
        "    my_model.parameters(),\n",
        "    lr=training_args[\"learning_rate\"],\n",
        ")\n",
        "\n",
        "for epoch in range(training_args[\"num_train_epochs\"]):\n",
        "    my_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = my_model(inputs).squeeze(1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    my_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = torch.sigmoid(my_model(inputs))\n",
        "            predicted = torch.round(outputs).squeeze(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    if (epoch) % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {running_loss/len(train_loader):.6f}')\n",
        "        print(f'Validation Accuracy: {100 * correct / total:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV10x90yJD16"
      },
      "source": [
        "# 5. Create Nada program that runs blind inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX5g5TBQYcHz"
      },
      "source": [
        "At this point, we have a trained PyTorch model.\n",
        "\n",
        "Now, we want to create a program that loads this model, loads some dataset and performs blind inference.\n",
        "\n",
        "Luckily we have just the tool for that: it's called Nada!\n",
        "\n",
        "We'll start off by writing a small .toml file that defines some basic configurations related to our project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zFbjXrwwJD16"
      },
      "outputs": [],
      "source": [
        "%%to_file nada-project.toml\n",
        "name = \"inference\"\n",
        "version = \"0.1.0\"\n",
        "authors = [\"\"]\n",
        "\n",
        "[[programs]]\n",
        "path = \"src/inference.py\"\n",
        "prime_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ksVXJM7YcHz"
      },
      "source": [
        "Now we can implement a program that:\n",
        "- Loads our brand new model, using `nada-ai`\n",
        "- Loads the dataset we want to run blind inference on, using `nada-numpy`\n",
        "- Runs blind inference\n",
        "- Returns the inference result, using the `nada-dsl`\n",
        "\n",
        "**TODO: üìù implement your own model using nada-ai modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fj6c74WzJD16"
      },
      "outputs": [],
      "source": [
        "%%to_file src/inference.py\n",
        "import nada_numpy as na\n",
        "\n",
        "from nada_ai import nn\n",
        "from nada_dsl import Output\n",
        "from typing import List\n",
        "\n",
        "INPUT_DIMS=(7,)  # we specify the dimensions of the input dataset\n",
        "\n",
        "# TODO: üìù implement your model using nada-ai modules\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.Linear(7, 16)\n",
        "        self.ln2 = nn.Linear(16, 1)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: na.NadaArray) -> na.NadaArray:\n",
        "        x = self.ln1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "def nada_main() -> List[Output]:\n",
        "    \"\"\"Main Nada program\"\"\"\n",
        "    # There will be two parties in this example: a party that\n",
        "    # provides the model and another party that provides a dataset\n",
        "    parties = na.parties(2)\n",
        "\n",
        "    # First, we randomly initialize the model we wish to load\n",
        "    my_model = MyModel()\n",
        "\n",
        "    # Next, we load the model weights as secrets from the network\n",
        "    # We assume the model weights are floats provided by the first party\n",
        "    # It will attempt to load a set of secrets named `my_model`\n",
        "    my_model.load_state_from_network(\n",
        "        \"my_model\",\n",
        "        parties[0],\n",
        "        na.SecretRational,\n",
        "    )\n",
        "\n",
        "    # Then, we load the dataset as secrets from the network\n",
        "    # We assume the dataset are floats provided by the second party\n",
        "    # It will attempt to load a set of secrets named `my_input`\n",
        "    my_input = na.array(\n",
        "        INPUT_DIMS,\n",
        "        parties[1],\n",
        "        \"my_input\",\n",
        "        na.SecretRational,\n",
        "    )\n",
        "\n",
        "    # Finally, we can run blind inference by simply providing the\n",
        "    # inference dataset to the dataset\n",
        "    result = my_model(my_input)\n",
        "\n",
        "    # We will return the result to the second party\n",
        "    return result.output(parties[1], \"my_output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSMxPGyYcHz"
      },
      "source": [
        "Now that we have our inference program, we can build it using the `nada` CLI tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h76SQK9jJD16",
        "outputId": "5f149b2b-f37a-4162-dea4-e047a8e55028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building program: \u001b[1m\u001b[32minference\u001b[39m\u001b[0m\n",
            "\u001b[1;32mBuild complete!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!nada build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejRPKApOV0ZP"
      },
      "source": [
        "# 6. Deploy our Nada inference program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOlYgvO6YcHz"
      },
      "source": [
        "Now that we have a set of instructions we want to execute in MPC (aka a built Nada program), we can proceed to deploying this model on the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U3QDAa_iJD16"
      },
      "outputs": [],
      "source": [
        "# We define some config variables\n",
        "\n",
        "home = os.getenv(\"HOME\")\n",
        "load_dotenv(f\"{home}/.config/nillion/nillion-devnet.env\")\n",
        "\n",
        "cluster_id = os.getenv(\"NILLION_CLUSTER_ID\")\n",
        "grpc_endpoint = os.getenv(\"NILLION_NILCHAIN_GRPC\")\n",
        "chain_id = os.getenv(\"NILLION_NILCHAIN_CHAIN_ID\")\n",
        "seed = \"my_seed\"\n",
        "\n",
        "userkey = UserKey.from_seed((seed))\n",
        "nodekey = NodeKey.from_seed((seed))\n",
        "\n",
        "client = create_nillion_client(userkey, nodekey)\n",
        "party_id = client.party_id\n",
        "user_id = client.user_id\n",
        "\n",
        "party_names = na_client.parties(2)\n",
        "program_name = \"inference\"\n",
        "program_mir_path = f\"target/{program_name}.nada.bin\"\n",
        "\n",
        "payments_config = create_payments_config(chain_id, grpc_endpoint)\n",
        "payments_client = LedgerClient(payments_config)\n",
        "payments_wallet = LocalWallet(\n",
        "    PrivateKey(bytes.fromhex(os.getenv(\"NILLION_NILCHAIN_PRIVATE_KEY_0\"))),\n",
        "    prefix=\"nillion\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdi5a1rEX1EH",
        "outputId": "2f3683da-e190-4aa5-e0fc-f6c9d3e1a102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting quote for operation...\n",
            "Submitting payment receipt 2 unil, tx hash 9CFFFFA2FE7DEBB0F426EEE650CB2A6B92C7AB9FD467CF3EB4FE8C5FCF5738E2\n",
            "Program deployed!\n",
            "program_id: 3rgqxWd47e171EUwe4RXP9hm45tmoXfuF8fC52S7jcFoQTnU8wPiL7hqWzyV1muak6bEg7iWhudwg4t2pM9XnXcp/inference\n"
          ]
        }
      ],
      "source": [
        "# Next we pay and provide a payment receipt to deploy our Nada program\n",
        "\n",
        "quote_store_program = await get_quote(\n",
        "    client, nillion.Operation.store_program(program_mir_path), cluster_id\n",
        ")\n",
        "\n",
        "receipt_store_program = await pay_with_quote(\n",
        "    quote_store_program, payments_wallet, payments_client\n",
        ")\n",
        "\n",
        "action_id = await client.store_program(\n",
        "    cluster_id, program_name, program_mir_path, receipt_store_program\n",
        ")\n",
        "\n",
        "program_id = f\"{user_id}/{program_name}\"\n",
        "\n",
        "print(\"Program deployed!\")\n",
        "print(\"program_id:\", program_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsAIUyNWZKUs"
      },
      "source": [
        "# 7. Provide data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hk3D9M7p7VG"
      },
      "source": [
        "Now that we have a deployed program, the input parties can provide the data that the program requires in order to run correctly.\n",
        "\n",
        "In our example, we need:\n",
        "- The first party to provide the model weights.\n",
        "- The second party to provide the inference data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hpWMLCt4rQcG"
      },
      "outputs": [],
      "source": [
        "permissions = nillion.Permissions.default_for_user(client.user_id)\n",
        "permissions.add_compute_permissions({client.user_id: {program_id}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j23ZO2bBYcH4"
      },
      "source": [
        "First, we'll let the first party supply the weights of the model we trained earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "J494MtSJrPZ-"
      },
      "outputs": [],
      "source": [
        "# We wrap the model in the relevant nada-ai client for the AI/ML framework we are using (here: PyTorch)\n",
        "model_client = TorchClient(my_model)\n",
        "\n",
        "# Now we can export the model state as secrets and name those secrets `my_model`\n",
        "model_secrets = nillion.NadaValues(\n",
        "    model_client.export_state_as_secrets(\"my_model\", na.SecretRational)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaLRxEtBYcH4",
        "outputId": "f9da0baf-da61-4dd7-e4da-92adb031e94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting quote for operation...\n",
            "Quote cost is 13922 unil\n",
            "Submitting payment receipt 13922 unil, tx hash 6FCE3967A2C819B8539185A1B542EDD3378AC7955EAFB7E0AF88F2CA2B37A5FE\n"
          ]
        }
      ],
      "source": [
        "# Finally, we can pay and store the model weights in the network as secrets\n",
        "receipt_store_model = await get_quote_and_pay(\n",
        "    client,\n",
        "    nillion.Operation.store_values(model_secrets, ttl_days=1),\n",
        "    payments_wallet,\n",
        "    payments_client,\n",
        "    cluster_id,\n",
        ")\n",
        "\n",
        "model_store_id = await client.store_values(cluster_id, model_secrets, permissions, receipt_store_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO38M_ZnYcH4"
      },
      "source": [
        "Next, we'll let the second party supply some input dataset\n",
        "\n",
        "**TODO: üìù provide your own input data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B4bWmaUdYcH4"
      },
      "outputs": [],
      "source": [
        "# TODO: üìù provide your own input data\n",
        "input_data = test_df.iloc[0].to_numpy()\n",
        "\n",
        "# We can export the data array as secrets and name those secrets `my_input`\n",
        "my_input = na_client.array(input_data, \"my_input\", na.SecretRational)\n",
        "input_secrets = nillion.NadaValues(my_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNOZg3RWrMqm",
        "outputId": "b896e783-36c4-4ef5-9d84-8ac0b65e0b3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting quote for operation...\n",
            "Quote cost is 770 unil\n",
            "Submitting payment receipt 770 unil, tx hash F379E971717549B153E1B8EFAAEC0F65C229B00EFEA74124430BD2DEF4A6FF7C\n"
          ]
        }
      ],
      "source": [
        "# Finally, we can pay and store the input data in the network as secrets\n",
        "receipt_store_data = await get_quote_and_pay(\n",
        "    client,\n",
        "    nillion.Operation.store_values(input_secrets, ttl_days=1),\n",
        "    payments_wallet,\n",
        "    payments_client,\n",
        "    cluster_id,\n",
        ")\n",
        "\n",
        "data_store_id = await client.store_values(cluster_id, input_secrets, permissions, receipt_store_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi2xQFuxrx9r"
      },
      "source": [
        "# 8. Run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEoncrbaYcH5"
      },
      "source": [
        "Finally, everything we wanted is in place:\n",
        "- We created & deployed a Nada program that loads a model named `my_model` and a dataset named `my_input`, runs inference and returns the result.\n",
        "- We trained a model and uploaded the weights to the network as `my_model`.\n",
        "- We defined a dataset we'd like to run inference on and uploaded it to the network as `my_input`.\n",
        "\n",
        "So, without further ado, we can actually execute the program now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m0H-RZC4rWTm"
      },
      "outputs": [],
      "source": [
        "compute_bindings = nillion.ProgramBindings(program_id)\n",
        "\n",
        "for party_name in party_names:\n",
        "    compute_bindings.add_input_party(party_name, party_id)\n",
        "\n",
        "compute_bindings.add_output_party(party_names[-1], party_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EFX13Jer7KQ",
        "outputId": "bdab2dcd-49f0-40c7-bb98-0a86abb7f2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting quote for operation...\n",
            "Quote cost is 388 unil\n",
            "Submitting payment receipt 388 unil, tx hash 02AFC87804D8827CEC66D78A632DF4DE179DDB8CEB344BF53ACE5AEC0E3AAECF\n",
            "‚úÖ Compute complete for compute_id 6e7658b5-ba68-437e-9d90-4fc3008e00ea\n",
            "Result is {'my_output_0': -2.356842041015625}\n"
          ]
        }
      ],
      "source": [
        "computation_time_secrets = nillion.NadaValues({})\n",
        "\n",
        "receipt_compute = await get_quote_and_pay(\n",
        "    client,\n",
        "    nillion.Operation.compute(program_id, computation_time_secrets),\n",
        "    payments_wallet,\n",
        "    payments_client,\n",
        "    cluster_id,\n",
        ")\n",
        "\n",
        "_ = await client.compute(\n",
        "    cluster_id,\n",
        "    compute_bindings,\n",
        "    [model_store_id, data_store_id],\n",
        "    computation_time_secrets,\n",
        "    receipt_compute,\n",
        ")\n",
        "\n",
        "while True:\n",
        "    compute_event = await client.next_compute_event()\n",
        "    if isinstance(compute_event, nillion.ComputeFinishedEvent):\n",
        "        print(f\"‚úÖ Compute complete for compute_id {compute_event.uuid}\")\n",
        "        result = compute_event.result.value\n",
        "        break\n",
        "\n",
        "result = {\n",
        "    key: na_client.float_from_rational(value)\n",
        "    for key, value in result.items()\n",
        "}\n",
        "print(\"Result is\", result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
