{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Before starting this notebook make sure that the kernel of the previous notebook is shutdown or reset it's state to forget the previous `model_user` Nillion client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If problems arise with the loading of the shared library, this script can be used to load the shared library before other libraries.\n",
    "## Remember to also run on your local machine the script below:\n",
    "# bash replace_lib_version.sh\n",
    "\n",
    "import platform\n",
    "import ctypes\n",
    "\n",
    "if platform.system() == \"Linux\":\n",
    "    # Force libgomp and py_nillion_client to be loaded before other libraries consuming dynamic TLS (to avoid running out of STATIC_TLS)\n",
    "    ctypes.cdll.LoadLibrary(\"libgomp.so.1\")\n",
    "    ctypes.cdll.LoadLibrary(\n",
    "        \"/home/vscode/.local/lib/python3.12/site-packages/py_nillion_client/py_nillion_client.abi3.so\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import py_nillion_client as nillion\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "import nada_algebra as na\n",
    "import nada_algebra.client as na_client\n",
    "import py_nillion_client as nillion\n",
    "from nillion_python_helpers import (\n",
    "    create_nillion_client,\n",
    "    getUserKeyFromFile,\n",
    "    getNodeKeyFromFile,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Nillion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Nillion network, we need to have a user key and a node key. These serve different purposes:\n",
    "\n",
    "The `user_key` is the user's private key. The user key should never be shared publicly, as it unlocks access and permissions to secrets stored on the network.\n",
    "\n",
    "The `node_key` is the node's private key which is run locally to connect to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all Nillion network environment variables\n",
    "assert os.getcwd().endswith(\n",
    "    \"examples/multi_layer_perceptron\"\n",
    "), \"Please run this script from the examples/multi_layer_perceptron directory otherwise, the rest of the tutorial may not work\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = os.getenv(\"NILLION_CLUSTER_ID\")\n",
    "model_user_userkey = getUserKeyFromFile(os.getenv(\"NILLION_USERKEY_PATH_PARTY_2\"))\n",
    "model_user_nodekey = getNodeKeyFromFile(os.getenv(\"NILLION_NODEKEY_PATH_PARTY_2\"))\n",
    "model_user_client = create_nillion_client(model_user_userkey, model_user_nodekey)\n",
    "model_user_party_id = model_user_client.party_id\n",
    "model_user_user_id = model_user_client.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program ID:  5GnDBJ4brQtS7U3YsUdweoKUfbN7t9CNDaZ7AnVLarwWcRNfQUwr1Ct2aTQsMhhvjb1xeYPfiKAahMz7i5D85ZUd/main\n",
      "Model Store ID:  d6002305-15f9-4e76-b5aa-510560f89e75\n",
      "Model Provider Party ID:  12D3KooWQLbKxRFoa3rcA8do1R2o96yhgxGnJS5RbTr3ZJwQiQay\n"
     ]
    }
   ],
   "source": [
    "# This information was provided by the model provider\n",
    "with open(\"data/tmp.json\", \"r\") as provider_variables_file:\n",
    "    provider_variables = json.load(provider_variables_file)\n",
    "\n",
    "program_id = provider_variables[\"program_id\"]\n",
    "model_store_id = provider_variables[\"model_store_id\"]\n",
    "model_provider_party_id = provider_variables[\"model_provider_party_id\"]\n",
    "\n",
    "print(\"Program ID: \", program_id)\n",
    "print(\"Model Store ID: \", model_store_id)\n",
    "print(\"Model Provider Party ID: \", model_provider_party_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model user flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")(Image.open(\"data/COVID-19_Lung_CT_Scans/COVID-19/COVID-19_0001.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 16, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_batch = np.array(test_image.unsqueeze(0))\n",
    "test_image_batch.shape  # (B, channels, H, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send features to Nillion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def store_images(\n",
    "    *,\n",
    "    client: nillion.NillionClient,\n",
    "    cluster_id: str,\n",
    "    program_id: str,\n",
    "    party_id: str,\n",
    "    user_id: str,\n",
    "    images: torch.Tensor,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Stores text features in Nillion network.\n",
    "\n",
    "    Args:\n",
    "        client (nillion.NillionClient): Nillion client that stores features.\n",
    "        cluster_id (str): Nillion cluster ID.\n",
    "        program_id (str): Program ID of Nada program.\n",
    "        party_id (str): Party ID of party that will store text features.\n",
    "        user_id (str): User ID of user that will get compute permissions.\n",
    "        images (torch.Tensor): Image batch.\n",
    "        precision (int): Scaling factor to convert float to ints.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Resulting `model_user_party_id` and `images_store_id`.\n",
    "    \"\"\"\n",
    "    secrets = nillion.Secrets(\n",
    "        na_client.array(images, \"my_input\", nada_type=na.SecretRational)\n",
    "    )\n",
    "\n",
    "    secret_bindings = nillion.ProgramBindings(program_id)\n",
    "    secret_bindings.add_input_party(\"Party1\", party_id)\n",
    "\n",
    "    images_store_id = await client.store_secrets(\n",
    "        cluster_id, secret_bindings, secrets, None\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model_user_user_id\": user_id,\n",
    "        \"images_store_id\": images_store_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Images uploaded successfully!\n",
      "model_user_user_id: unDxCapdG2Dp7w2FwajbBWEF6wrinZp1ArKPvqeMxGt32WbkoXcZGQcSJHwDUMvKr4AG6zQnW4GGDaBcCFqtsu3\n",
      "images_store_id: 3465b7fe-062e-4528-9fc3-e20bdff86008\n"
     ]
    }
   ],
   "source": [
    "result_store_features = await store_images(\n",
    "    client=model_user_client,\n",
    "    cluster_id=cluster_id,\n",
    "    program_id=program_id,\n",
    "    party_id=model_user_party_id,\n",
    "    user_id=model_user_user_id,\n",
    "    images=test_image_batch,\n",
    ")\n",
    "\n",
    "model_user_user_id = result_store_features[\"model_user_user_id\"]\n",
    "images_store_id = result_store_features[\"images_store_id\"]\n",
    "\n",
    "print(\"✅ Images uploaded successfully!\")\n",
    "print(\"model_user_user_id:\", model_user_user_id)\n",
    "print(\"images_store_id:\", images_store_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference & check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_inference(\n",
    "    *,\n",
    "    client: nillion.NillionClient,\n",
    "    cluster_id: str,\n",
    "    program_id: str,\n",
    "    model_user_party_id: str,\n",
    "    model_provider_party_id: str,\n",
    "    model_store_id: str,\n",
    "    images_store_id: str,\n",
    ") -> Dict[str, str | float]:\n",
    "    \"\"\"Runs blind inference on the Nillion network by executing the Nada program on the uploaded data.\n",
    "\n",
    "    Args:\n",
    "        client (nillion.NillionClient): Nillion client that runs inference.\n",
    "        cluster_id (str): Nillion cluster ID.\n",
    "        program_id (str): Program ID of Nada program.\n",
    "        model_user_party_id (str): Party ID of party that will run inference.\n",
    "        model_user_party_id (str): Party ID of party that will provide model params.\n",
    "        model_store_id (str): Store ID that points to the model params in the Nillion network.\n",
    "        images_store_id (str): Store ID that points to the images in the Nillion network.\n",
    "        precision (int): Scaling factor to convert float to ints.s\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str | float]: Resulting `compute_id`, `output_0` and `output_1`.\n",
    "    \"\"\"\n",
    "    compute_bindings = nillion.ProgramBindings(program_id)\n",
    "    compute_bindings.add_input_party(\"Party0\", model_user_party_id)\n",
    "    compute_bindings.add_input_party(\"Party1\", model_provider_party_id)\n",
    "    compute_bindings.add_output_party(\"Party1\", model_user_party_id)\n",
    "\n",
    "    _ = await client.compute(\n",
    "        cluster_id,\n",
    "        compute_bindings,\n",
    "        [images_store_id, model_store_id],\n",
    "        nillion.Secrets({}),\n",
    "        nillion.PublicVariables({}),\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        compute_event = await client.next_compute_event()\n",
    "        if isinstance(compute_event, nillion.ComputeFinishedEvent):\n",
    "            inference_result = compute_event.result.value\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"compute_id\": compute_event.uuid,\n",
    "        \"output_0\": inference_result[\"my_output_0_0\"] / (2 ** na.get_log_scale()),\n",
    "        \"output_1\": inference_result[\"my_output_0_1\"] / (2 ** na.get_log_scale()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_id': '42b1bcc4-fa08-4260-aa2e-213fb9702f10',\n",
       " 'output_0': -1.40350341796875,\n",
       " 'output_1': 0.935302734375}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_inference = await run_inference(\n",
    "    client=model_user_client,\n",
    "    cluster_id=cluster_id,\n",
    "    program_id=program_id,\n",
    "    model_user_party_id=model_user_party_id,\n",
    "    model_provider_party_id=model_provider_party_id,\n",
    "    model_store_id=model_store_id,\n",
    "    images_store_id=images_store_id,\n",
    ")\n",
    "result_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare result to what we would have gotten in plain-text inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom torch Module\n",
    "class MyNN(torch.nn.Module):\n",
    "    \"\"\"My simple neural net\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Model is a two layers and an activations\"\"\"\n",
    "        super(MyNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1, out_channels=2, kernel_size=3, stride=4, padding=1\n",
    "        )\n",
    "        self.pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features=8, out_features=2)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"My forward pass logic\"\"\"\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyNN()\n",
    "my_model.load_state_dict(torch.load(\"./data/my_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0880, 0.9120], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(my_model(test_image.unsqueeze(0))[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0880, 0.9120])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(\n",
    "    torch.Tensor([result_inference[\"output_0\"], result_inference[\"output_1\"]]), dim=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
